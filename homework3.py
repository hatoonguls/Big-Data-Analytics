# -*- coding: utf-8 -*-
"""homework3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HEghEIk6GCw4XZzbBLDmPdhEJmjsRc74

# CIS 5450 Homework 3: Spark SQL

### **Deadline: Monday, 23rd, October 2023 @ 10pm EST**

#### **Worth 100 points**

Welcome to CIS 5450 Homework 3! In this homework you will gain a mastery of using Spark SQL. By the end, you'll be a star (not that you aren't already one). Over the next few days you will be using an EMR cluster to use Spark to manipulate the datasets about Amazon products and their reviews.

## The Necessary Notes and Nags
Before we begin here are some important notes to keep in mind,


1.   **IMPORTANT!** I said it twice, it's really important. In this homework, we will be using AWS resources. You are given a quota ($100) to use for the entirety of the homework. There is a small chance you will use all this money, however it is important that at the end of every session, you **shut down your EMR cluster**.
2.   **Be sure you use Google Colab for this Homework** since we must connect to the EMR cluster and local Jupyter will have issues doing that. Using a Google Colab Notebook with an EMR cluster has two important abnormalities:
    * The first line of any cell in which you will use the spark session must be `%%spark`. Notice that all cells below have this.
    * You will, unfortunately, not be able to stop a cell while it is running. If you wish to do so, you will need to restart your cluster. See the Setup EMR Document for reference.
3.   You are **required** to use Spark SQL queries to handle the data in the assignment. Mastering SQL is more beneficial than being able to use Spark commands (functions) as it will show up in more areas of programming and data science/analytics than just Spark. Use the following [function list](https://spark.apache.org/docs/latest/api/sql/index.html#) to see all the SQL functions avaliable in Spark.
4.   Throughout the homework you will be manipulating Spark dataframes (sdfs).
5. Based on the challenges you've faced in the previous homework, we are including information on the expected schema of your results.  Apache Spark is very fiddly but we hope this will help.
6. There are portions of this homework that are _very_ hard. We urge you start early to come to office hours and get help if you get stuck. But don't worry, I can see the future, and you all got this.


With that said, let's dive in.

## **You MUST check that your notebook displays ALL visualizations on the Gradescope preview AND verify that the autograder finishes running and gives you your expected score (not a 0).**

## Step 0: Set up EMR

Follow the [AWS Academy Getting Started](https://docs.google.com/document/d/1xjSx730iSq9YpZQ-JpE4hcFzhTb8mRWxyEIveRIe8kk/edit?usp=sharing) instructions.

Move on to Step 0.1 after you have completed all the steps in the document.

![ACME GIANT RUBBER BAND](https://pbs.twimg.com/media/DRqbJh7UMAE2z4o?format=jpg&name=4096x4096)

### 0.1: The Superfluous Setup

Run the following two cells. These will allow your colab notebook to connect to and use your EMR.
"""

# Commented out IPython magic to ensure Python compatibility.
# %set_env HW_ID=CIS5450_F23_HW3

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install penngrader-client

!apt install libkrb5-dev
!pip install sparkmagic

# Commented out IPython magic to ensure Python compatibility.
# %load_ext sparkmagic.magics

"""### 0.2: The Sharp Spark

Now, connect your notebook to the EMR cluster you created. In the first cell, copy the link to the Master Public DNS specified in the setup document. You will need to add `http://` to the beginning of the address and the auth details to the end.

For example, if my DNS (directly from the AWS EMR console) is `ec2-3-15-237-211.us-east-2.compute.amazonaws.com` my address would be,

`http://ec2-3-15-237-211.us-east-2.compute.amazonaws.com -a cis545-livy -p password1 -t Basic_Access`

Insert this in the `# TODO # below`. For our example, the cell would read,

```
%spark add -s spark_session -l python -u http://ec2-3-15-237-211.us-east-2.compute.amazonaws.com -a cis545-livy -p password1 -t Basic_Access
```

**READ:** If your Spark setup times out during the first time running the cell, then you will need to rerun the cell. The message below indicates that Spark was implemented successfully.

***SparkSession available as 'spark'.***
"""

# Commented out IPython magic to ensure Python compatibility.
# TODO: Copy the line above, enter your Master Public DNS with the proper formatting and host, and update the password
# %spark add -s spark_session -l python -u http://ec2-18-212-57-244.compute-1.amazonaws.com -a cis545-livy -p apple123 -t Basic_Access

#%spark delete -s spark_session

# If you ever need to restart, you may need to...
#%spark delete -s spark_session
#OR just factory reset runtime under the runtime tab
# %spark delete -s spark_session

"""Enter your 8-digit Penn Key as an integer in the cell
below. This will be used in the autograder.  **Please also update the cell below, with the same ID!**
"""

from penngrader.grader import *
import os
import pandas as pd

# PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY.
# IF NOT, THE AUTOGRADER WON'T KNOW WHO TO ASSIGN POINTS TO YOU IN OUR BACKEND
# YOUR PENN-ID GOES HERE AS AN INTEGER
STUDENT_ID = 16716394

# You should also update this to a unique "secret" just for this homework, to
# authenticate this is YOUR submission
SECRET = STUDENT_ID

# Commented out IPython magic to ensure Python compatibility.
# %%writefile notebook-config.yaml
# 
# grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'
# grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'

grader = PennGrader('notebook-config.yaml', os.environ['HW_ID'], STUDENT_ID, SECRET)

"""Run the above cells to setup the autograder. Make sure to have set your 8 digit Penn ID in the cell above. It will also import all the modules you need for the homework.

## Step 1: Data Wrangling, Cleaning, and Shaping [36 pts]

In this homework we will be working with the goodreads review data from kaggle. This dataset contains two files one with the reviews for all the books and details of the reviews. The other contains more information about the books such authors, publishers and categories

The data you will use is stored in an S3 bucket, a cloud storage service. Below, with our help, you will download it onto the nodes of your [EMR cluster](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html).

### 1.1: The Stupendous Schema

When loading data, Spark will try to infer its structure on its own. This process is faulty because Spark will sometimes infer the type incorrectly. Spark's ability to determine types is not reliable, thus you will need to define a schema for both the reviews and the book dataframes.

A schema is a description of the structure of data. In Spark, schemas are defined using a `StructType` object. This is a collection of data types, termed `StructField`'s, that specify the structure and variable type of each component of the dataset. For example, suppose we have the following csv file,


```
id,name,score1,score2,total
1,naruto,0,0.5,0.5
2,sasuke,50,50,100
3,sakura,100,100,200
```

We would define its schema as follows,

```       
schema = StructType([
           StructField("id", IntegerType()),
           StructField("name", StringType(), nullable=True),
           StructField("score1", FloatType(), nullable =True),
           StructField("score2", FloatType(), nullable =True),
           StructField("total", FloatType(), nullable =True)
         ])
```


Each `StructField` has the following structure: `(name, type, nullable)`. The `nullable` flag defines that the specified field may be empty. Your first task is to define the `schema` of `Books_ratings.csv`. A smaller version of the csv dataset can be found [here](https://drive.google.com/file/d/1nJEdSnp3Vwvfb4qMPWaLRIw6jelb7D8X/view?usp=drive_link). Look at how this csv dataset looks, the fields and their types.



You will now be defining an explicit schema for the `Books_rating.csv` dataset. We have defined most of the fields so you can compare how the schema looks with the JSON dataset. Your Task will be to define the schema for the following fields - `profileName,review/helpfulness, review/score, review/time, review/summary and review/text`.

Make sure to use `nullable=True` for all the fields as well as **store review/helpfulness as a StringType()**.

Note: There is also no grading cell for this step. But your csv file won't load in section 1.2.1 if it's wrong, so you have a way of testing.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# from pyspark.sql.types import *
# 
# # TODO: Finish defining the linkedin_small_real.json schema
# # We've provided most of the fiddly details, but you'll
# # need to fill in **name**, ** experience ** and **events** fields!
# 
# ratings_schema = StructType([
#      StructField("Id", IntegerType(), nullable=True),
#      StructField("Title", StringType(), nullable=True),
#      StructField("Price", FloatType(), nullable=True),
#      StructField("User_id", StringType(), nullable=True),
#      StructField("profileName", StringType(), nullable=True),
#      StructField("review/helpfulness", StringType(), nullable=True),
#      StructField("review/score", StringType(), nullable=True),
#      StructField("review/time", StringType(), nullable=True),
#      StructField("review/summary", StringType(), nullable=True),
#      StructField("review/text", StringType(), nullable=True)
# 
# 
# 
# 
#     # TODO: fill in the necessary structure for profileName,review/helpfulness, review/score, review/time, review/summary and review/text
# 
# 
# ])
#

"""### 1.2: The Langorous Load [12 Pts]

#### 1.2.1: Load Books rating Dataset [4 Pts]

In the following cell, you will load the `Books_rating.csv` dataset from your S3 bucket into a Spark dataframe (sdf) called `goodreads_book_ratings_sdf`. If you have constructed `schema` correctly then `spark.read.json()` will read in the dataset. ***You do not need to edit this cell***.

If this doesn't work, go back to the prior cell and update your `schema`!

Note that the cell below will load data even if your schema is incomplete and has left out some columns of the data, so be sure to check that you have included all of the fields from the JSON.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# from pyspark import SparkFiles
# review_url = "https://penn-cis545-files.s3.amazonaws.com/Books_rating.csv"
# sc.addFile(review_url)
# path  = SparkFiles.get('download')
# goodreads_review_data_sdf = spark.read.csv('file://'+SparkFiles.get('Books_rating.csv'),header=True,schema = ratings_schema,quote='\"',escape='\"')

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# # Let's print out the first few rows to see how the data looks like in tabular form
# #goodreads_review_data_sdf.show(5)

"""The cell below shows how to run SQL commands on Spark tables. **Use this as a template for all your SQL queries in this notebook.**

For almost all the questions you will need to create a temporary view using `createOrReplaceTempView`, then write your sql `query` as a string and then run the query on spark using `spark.sql(query)`. To see what your query resulted use `.show()`.

***You do not need to edit this cell***.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# # Create SQL-accesible table
# goodreads_review_data_sdf.createOrReplaceTempView("ratings_data")
# 
# # Declare SQL query to be excecuted
# query = '''SELECT *
#            FROM ratings_data
#            ORDER BY Id
#            LIMIT 10'''
# 
# # Save the output sdf of spark.sql() as answer_sdf
# answer_sdf = spark.sql(query)
# #answer_sdf.show()

"""We can then copy the `answer_sdf` to Colab to submit to PennGrader..."""

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o answer_sdf

## AUTOGRADER
grader.grade(test_case_id = 'test_schema_reviews', answer = answer_sdf)

"""#### 1.2.2: SQL refresher [4 Pts]

In the next cell, we want you to create `ratings_cleaned_df` to fetch the data from the `ratings_data` table created above. The names are a bit hard to work with as they have the '/' character in them. So we will be changing the name of the columns. We will make the following changes
1. review/helpfulness - helpfulness
2. review/score - score
3. review/summary - summary
4. review/text - text
5. review/time - time

Change the names of these columns and save the dataframe in ratings_cleaned_df with schema `(Id, Title, User_id, helpfulness,score, summary, text, time)`. Remove all NULLs from the `Id, Title` and the `score` columns. Sort the columns by `Id` and `User_id`, all ascending order.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# #TODO
# 
# query = '''
# SELECT
#   Id,
#   Title,
#   User_id,
#   `review/helpfulness` AS helpfulness,
#   `review/score` AS score,
#   `review/summary` AS summary,
#   `review/text` AS text,
#   `review/time` AS time
# FROM ratings_data
# WHERE Id is not NULL AND Title is not NULL AND `review/score` is not NULL
# ORDER BY Id, User_id
# '''
# 
# ratings_cleaned_df = spark.sql(query)
# #ratings_cleaned_df.show()

# Commented out IPython magic to ensure Python compatibility.
# #################     DO NOT EDIT      ##################
# %%spark
# ratings_cleaned_df.createOrReplaceTempView("test_1_2_2")
# test_1_2_2_sdf = spark.sql("SELECT * FROM test_1_2_2 LIMIT 100")

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o test_1_2_2_sdf

## AUTOGRADER
grader.grade(test_case_id = 'test_cleaned_reviews', answer = test_1_2_2_sdf)

"""#### 1.2.3: Load Book Data [4 Pts]

Just the way you created a schema for the review dataset, now create a schema for the book data. A tiny version of the data is [here](https://drive.google.com/file/d/1Z31KCKEgAc0l4WgUuRCm6JshB5cdvNKx/view?usp=drive_link) in csv format, so you can see what the types should be for the different fields (columns in the csv). Keep the authors and category columns as `StringType()`. Keep all links as `StringType()` as well. We will work to convert this into the actual data type we need later on.

Make sure to make all columns nullable!

"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# from pyspark.sql.types import *
# 
# # TODO
# 
# data_schema = StructType([
#          StructField("Title", StringType(), nullable=True),
#      StructField("description", StringType(), nullable=True),
#      StructField("authors", StringType(), nullable=True),
#      StructField("image", StringType(), nullable=True),
#      StructField("previewLink", StringType(), nullable=True),
#      StructField("publisher", StringType(), nullable=True),
#      StructField("publishedDate", StringType(), nullable=True),
#      StructField("infoLink", StringType(), nullable=True),
#      StructField("categories", StringType(), nullable=True),
#      StructField("ratingsCount", IntegerType(), nullable=True)
# 
# ])

"""Now we will remove the Null values in the Title and Categories column and also drop our `previewLink,publishedDate,infoLink, description and ratingsCount` columns. We will also order the dataframe by Title in ascending order. The resulting dataframe is very noisy but in the following sections, we will clean our dataframe so that it can be used for analysis.

Return your final dataframe as `books_cleaned_df` with the following schema

**Final Schema**:
>Title | authors | image | publisher | categories |
>--- | --- | --- | --- | --- |
"""

# Commented out IPython magic to ensure Python compatibility.
# #################     DO NOT EDIT      ##################
# %%spark
# spark.conf.set("spark.sql.legacy.timeParserPolicy","LEGACY")
# # Load books data
# 
# # stocks_sdf = spark.read.format("csv") \
# #               .option("header", "true") \
# #               .schema(stocks_schema) \
# #               .load("s3a://penn-cis545-files/stocks.csv")
# 
# data_url = "https://penn-cis545-files.s3.amazonaws.com/books_data.csv"
# sc.addFile(data_url)
# path  = SparkFiles.get('download')
# goodreads_book_data_sdf = spark.read.csv('file://'+SparkFiles.get('books_data.csv'),header=True,schema = data_schema,quote='\"',escape='\"')
# 
# # Creates SQL-accesible table
# goodreads_book_data_sdf.createOrReplaceTempView('book_data')

"""Now we will remove the Null values in the Title and Categories column and also drop our `previewLink,publishedDate,infoLink and ratingsCount` columns. We will also order the dataframe by Title in ascending order. The resulting dataframe is very noisy but in the following sections, we will clean our dataframe so that it can be used for analysis.

Return your final answer with the following schema

**Final Schema**:
>Title | authors | image | publisher | categories |
>--- | --- | --- | --- | --- |
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# #TODO
# 
# query = '''
# SELECT
#   Title,
#   authors,
#   image,
#   publisher,
#   categories
# FROM book_data
# WHERE Title is not NULL AND categories is not NULL
# ORDER BY Title
# '''
# 
# books_cleaned_df = spark.sql(query)
# #books_cleaned_df.show(10)

# Commented out IPython magic to ensure Python compatibility.
# #################     DO NOT EDIT      ##################
# %%spark
# books_cleaned_df.createOrReplaceTempView("test_1_2_3")
# test_1_2_3_sdf = spark.sql("SELECT * FROM test_1_2_3 LIMIT 100")

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o test_1_2_3_sdf

## AUTOGRADER
grader.grade(test_case_id = 'test_schema_books', answer = test_1_2_3_sdf)

"""### 1.3 Cleaning ratings_cleaned_df [12 Pts]

Now that we have loaded the dataframes we will be working with, we can go ahead and clean our dataframes. We will first start with the `ratings_cleaned_df` dataframe

#### 1.3.1: Create columns for helpfulness [4 Pts]

In the `ratings_cleaned_df` dataframe, currently the helpfulness column is a string. We want to quantify this information. In the next section we will convert this column into two different columns - `no_of_reviews` containing the number of helpful tags the review got (denominator), and `positive_reviews` containing the number of total tags the review got (numerator). Cast both of these columns in to type `float`. You do not need to use sparkSQL in this section.

Return the dataframe as `ratings_cleaned_df`. The final schema should be

**Final Schema**:
>Id | Title | helpfulness | score | summary | text | no_of_reviews | positive_reviews | time
>--- | --- | --- | --- | --- | --- | --- | --- | --- |

Hint :  Taking a look at the [split](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.split.html) function might be useful here
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# from pyspark.sql.functions import split, col
# 
# 
# 
# #ToDo
# 
# ratings_cleaned_df = ratings_cleaned_df.withColumn('positive_reviews', split(ratings_cleaned_df.helpfulness,'/').getItem(0).cast('float')).withColumn('no_of_reviews', split(ratings_cleaned_df.helpfulness,'/').getItem(1).cast('float'))
# #ratings_cleaned_df.show(10)

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o ratings_cleaned_df

## AUTOGRADER
grader.grade(test_case_id = 'test_helpful_tags', answer = ratings_cleaned_df)

"""#### 1.3.2: Calculate helpfulness score [4 Pts]

Using the `ratings_cleaned_df` dataframe, we will now calculate the helpfulness score. Here, we define it as the ratio of positive tags to the total number of tags that a review recieved. To do this, we will be using a udf.

A udf is defined as a normal Python function and then registered to be used as a Spark SQL function. Your task is to create a udf, `calc_score()` that will calculate the helpfulness score.

Fill out the function `calc_score()` below. Then use `spark.udf.register()` to register it as a SQL function. The command is provided. ***You do not need to edit it***. Note, we have defined the udf as returning `FloatType()`. Ensure that your function returns this. ***Your function should return Null if it is not divisible***. Similar to the previous section, you do not need to use SparkSQL to run this section.

The final output should contain the same columns along with the `helpfulness_score` column. Return the `ratings_cleaned_df` with the following schema -

**Final Schema**:
>Id | Title | helpfulness | score | summary | text | no_of_reviews | positive_reviews | helpfulness_score
>--- | --- | --- | --- | --- | --- | --- | --- | --- |
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# # Remember to specify the arguments that are passed into the function
# def calc_score(total_tags, positive_tag):
#   if total_tags != 0:
#     x = positive_tag/total_tags
#   else:
#     x = 'nan'
#   return x
#

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# from pyspark.sql.functions import udf
# #Use the line below to register your udf
# 
# calc_score_udf = udf(calc_score,FloatType())
# 
# #ToDo
# ratings_cleaned_df = ratings_cleaned_df.withColumn('helpfulness_score', calc_score_udf('no_of_reviews', 'positive_reviews'))
# ratings_cleaned_df = ratings_cleaned_df.drop("User_id")
# 
# #ratings_cleaned_df.show(10)

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o ratings_cleaned_df

## AUTOGRADER
grader.grade(test_case_id = 'test_helpfulness_score', answer = ratings_cleaned_df)

"""#### 1.3.3 Convert html symbols to normal symbols [4 Pts]

The `text` and `summary` columns are scraped from html sources. They contain named and numeric character references such as `&amp` for &, `&quot` for " and so on. In this section we will convert these named and numeric character references to their corresponding Unicode characters.

**Be sure to handle the case when NoneType is provded to the function**. If this is not taken care of, there will be problems later on in the homework when you filter these tables further. Your udf must return `None` if it is provided type `None` as input.

Register the udf and apply the udf on your summary and text columns.

Details of the html module can be found [here](https://docs.python.org/3/library/html.html)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# import html
# 
# #ToDo
# 
# def clean_html_codes(s):
#   if s == None:
#     return None
#   else:
#     return html.unescape(s)
#

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# #ToDo register the clean_html_codes udf and clean the summary and text columns with it
# from pyspark.sql.functions import udf
# #Use the line below to register your udf
# 
# html_unescape_udf = udf(clean_html_codes, StringType())
# 
# ratings_cleaned_df = ratings_cleaned_df.withColumn('text', html_unescape_udf('text')).withColumn('summary', html_unescape_udf('summary'))
# 
# #ratings_cleaned_df.show(10)
#

# Commented out IPython magic to ensure Python compatibility.
# #################     DO NOT EDIT      ##################
# %%spark
# ratings_cleaned_df.createOrReplaceTempView("test_1_3_3")
# test_1_3_3_sdf = spark.sql("SELECT * FROM test_1_3_3 LIMIT 200")

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o test_1_3_3_sdf

## AUTOGRADER
grader.grade(test_case_id = 'test_html_tags', answer = test_1_3_3_sdf)

"""### 1.4: Cleaning Book Data [12 Pts]

#### 1.4.1: Convert author and category columns to *arrays* [6 Pts]

We imported authors and categories in the `books_cleaned_df` as StringType. However, on inspection we actually see that it is an array of values. In this section we will convert both of these columns into array of strings. It might be useful to look at these functions to figure how you might go about these - [regexp_replace](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.regexp_replace.html) and [split](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.split.html)

Regexp_replace searches for the expression you provide and also allows you to replace your matched expression. Split on the other hand converts the string into a list, seprated by the delimiter. Combine these two functions to get the final output we want.

Your final dataframe should include all columns of the original `books_cleaned_df` but now with `authors` and `categories` as `array<string>` instead of `StringType()`
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# from pyspark.sql.functions import regexp_replace, col
# #TODO
# 
# books_cleaned_df = books_cleaned_df.withColumn('authors', split(regexp_replace('authors', '[\[\]]', ''), ','))
# 
# books_cleaned_df = books_cleaned_df.withColumn('categories', split(regexp_replace('categories', '[\[\]]', ''), ','))
# #books_cleaned_df.show(10)

# Commented out IPython magic to ensure Python compatibility.
# #################     DO NOT EDIT      ##################
# %%spark
# books_cleaned_df.createOrReplaceTempView("test_1_4_1")
# test_1_4_1_sdf = spark.sql("SELECT * FROM test_1_4_1 LIMIT 100")

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o test_1_4_1_sdf

## AUTOGRADER
grader.grade(test_case_id = 'test_book_array', answer = test_1_4_1_sdf)

"""#### 1.4.2 Explode the categories and author columns [2 Pts]

To make calculations easy for us, we will explode the dataframe on both the author and the category columns. At the end of this operation, each row corresponds to one author and one category for a given title. Thus, a title can appear multiple times in the final dataframe
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# from pyspark.sql.functions import explode
# 
# # TODO
# 
# books_cleaned_df = books_cleaned_df.withColumn('authors', explode('authors')).withColumn('categories', explode('categories'))
#

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o books_cleaned_df

## AUTOGRADER
grader.grade(test_case_id = 'test_book_expanded', answer = books_cleaned_df)

"""#### 1.4.3 Remove extra quotation marks in Title [4 Pts]

At the end of this entire mammoth of cleaning operations, we see that we are still left with quotes in the `Title,authors` and `categories` column. This is uneeded and might hinder our further operations. We will now remove the extra quotes in these columns and present our final iteration of the `books_cleaned_df`. We can use the `regexp_replace` function that we learnt about in the previous sections again here.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# books_cleaned_df = books_cleaned_df.withColumn('Title', regexp_replace('Title', '"', '')).withColumn('authors', regexp_replace('authors', "'", '')).withColumn('categories', regexp_replace('categories', "'", ''))

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o books_cleaned_df

## AUTOGRADER
grader.grade(test_case_id = 'test_cleaned_df', answer = books_cleaned_df)

"""## Step 2: Analyzing Books Data [10 pts]

This section will focus on the data in `books_cleaned_df`.

First, we should store this dataframe on Spark so that we can access it in our Spark SQL queries. Name the `books_cleaned_df` on Spark as `books_cleaned_data`. We will also drop the `image` and `ratingsCount` columns since they will not be used in this homework
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# books_cleaned_df = books_cleaned_df.drop("image", "ratingsCount")
# books_cleaned_df.createOrReplaceTempView("books_cleaned_data")
# #books_cleaned_df.show(10)

"""### 2.1 - Count number of authors per book [2 pts]

Currently, `books_cleaned_df` contains a row for each unique combination of `Title`, `authors`, and `categories`. We would like to find the number of authors per reference book.


**TODO**:


*   Use Spark SQL to find the number of authors per title in `books_cleaned_data`. Store this result in a column called `author_count`
*   Filter for `Reference` books
*   Sort the values by `Title` in alphabetical order
*   Save the result as `author_count_sdf`


**Final Schema**:
>Title | Author Count
>--- | ---

"""

# Commented out IPython magic to ensure Python compatibility.
# # TODO: Use Spark SQL to find the number of authors per title in books_cleaned_data. Store this result in a column called author_count
# # TODO: Filter for Reference books
# # TODO: Sort the values by Title in alphabetical order
# # TODO: Save the result as author_count_sdf
# %%spark
# query = '''
# SELECT
#   Title,
#   Count(DISTINCT authors) AS author_count
# FROM books_cleaned_data
# WHERE categories = 'Reference'
# GROUP BY Title
# ORDER BY Title
# '''
# 
# author_count_sdf = spark.sql(query)
# #author_count_sdf.show(5)

# Commented out IPython magic to ensure Python compatibility.
# #################     DO NOT EDIT      ##################
# %%spark
# author_count_sdf.createOrReplaceTempView("test_2_1")
# test_2_1_sdf = spark.sql("SELECT * FROM test_2_1")

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o test_2_1_sdf

# 2 points
grader.grade(test_case_id = 'test_author_count', answer = test_2_1_sdf)

"""### 2.2 - Find the most common category per publisher [4 pts]

**TODO**:


*   Use Spark SQL to find the most common category of books per publisher.
*   Store the most common category in a column called `top_category` and the number of books falling under that category in a column called `count`
*   Sort the rows by `count` in descending order, then `publisher` in alphabetical order
*   Save the result as `top_publisher_category_sdf`

The final result should have one row for each publisher. If there are multiple categories tied with the top count, then include a row for each category. Leave the percentages as decimals from 0-1

**Hints**:
*   A row in `books_cleaned_data` with the same publisher/category pair can have multiple rows if that title had multiple authors. Be sure to not double count because of the authors column.
*   The use of a temporary table (ex: WITH temp AS (...)) can be useful to break up your computation and store intermediate results

**Final Schema**:
>publisher | top_category | count
>--- | --- | ---


"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# query = '''
# WITH publisher_category AS (
#   SELECT DISTINCT publisher, categories, Title
#   FROM books_cleaned_data
#   ),
#   category_count AS (
#     SELECT publisher, categories, COUNT(*) AS count
#     FROM publisher_category
#     GROUP BY publisher, categories
#   ),
#   max_category_count AS (
#     SELECT publisher, MAX(count) as max_count
#     FROM category_count
#     GROUP BY publisher
#   )
#   SELECT c.publisher, c.categories as top_category, c.count
#   FROM category_count c
#   JOIN max_category_count m
#   ON c.publisher = m.publisher AND c.count = m.max_count
#   ORDER BY c.count DESC, c.publisher
# '''
# 
# top_publisher_category_sdf = spark.sql(query)

# Commented out IPython magic to ensure Python compatibility.
# #################     DO NOT EDIT      ##################
# %%spark
# top_publisher_category_sdf.createOrReplaceTempView("test_2_2")
# test_2_2_sdf = spark.sql("SELECT * FROM test_2_2")

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o test_2_2_sdf

# 4 points
grader.grade(test_case_id = 'test_top_publisher_category', answer = test_2_2_sdf)

"""### 2.3 - Find the distribution of books in three categories for each publisher [4 pts]

We are interested in seeing each publisher's distribution of titles that fall under the `Fiction`, `History`, and `Religion` categories

**TODO**:

*   Use Spark SQL to find the percentage of titles from each publisher that fall under the categories of `Fiction`, `History`, and `Religion`. Store these percentages in columns called `fiction`, `history`, and `religion`, respectively.
*   Also create a column called `title_count` which has the total number of titles from a publisher.
*   Drop rows where the publisher is NULL
*   Sort the rows by `title_count` in descending order and `publisher` in alphabetical order
*   Save the result as `categories_percentages_sdf`

The final result should have one row for each publisher. Leave the percentages as decimals from 0-1

**Hint:**
*   The CASE WHEN ... END ... function in SQL will be helpful in creating columns for the three categories
*   Similar to above, be sure not to double count authors


**Final Schema**:
>publisher | title_count | fiction | history | religion
>--- | --- | --- | --- | ---


"""

# Commented out IPython magic to ensure Python compatibility.
# # TODO: Use Spark SQL to find the percentage of titles from each publisher that fall under the categories of fiction, history, and religion.
# # Store these percentages in columns called fiction, history, and religion respectively.
# # TODO: Sort the rows by publisher in alphabetical order
# # TODO: Save the result as categories_percentages_sdf
# %%spark
# 
# query = '''
#   SELECT
#     publisher,
#     COUNT(DISTINCT title) as title_count,
#     AVG(CASE WHEN categories = 'Fiction' THEN 1 ELSE 0 END) as fiction,
#     AVG(CASE WHEN categories = 'History' THEN 1 ELSE 0 END) as history,
#     AVG(CASE WHEN categories = 'Religion' THEN 1 ELSE 0 END) as religion
#   FROM books_cleaned_data
#   WHERE publisher is not null
#   GROUP BY publisher
#   ORDER BY title_count DESC, publisher
# 
# '''
# 
# categories_percentages_sdf = spark.sql(query)
# #categories_percentages_sdf.show(5)

# Commented out IPython magic to ensure Python compatibility.
# #################     DO NOT EDIT      ##################
# %%spark
# categories_percentages_sdf.createOrReplaceTempView("test_2_3")
# test_2_3_sdf = spark.sql("SELECT * FROM test_2_3")

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o test_2_3_sdf

# 5 points
grader.grade(test_case_id = 'test_categories_percentages', answer = test_2_3_sdf)

"""## Step 3: Analyzing Books Reviews Data [19 pts]

Next, lets have a look at our Reviews data set. Now that we have cleaned a lot the data, lets start analyzing it. A good place to start is to gain a high level understanding of what our data looks like.

To begin with, lets store our dataframe on Spark so that we can access it during our Spark SQL queries. Name the `ratings_cleaned_df` on Spark as `ratings_cleaned_data`.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# # TODO: Create the ratings_cleaned_data table on Spark
# # Hint: Remember what every cell must include at the start - otherwise, the results may not be correct
# ratings_cleaned_df.createOrReplaceTempView('ratings_cleaned_data')

"""Now, lets figure out how many rows we are working with."""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# # TODO: Create a variable named cleaned_data_count that counts the total number of rows in the dataset
# # Hint: Remember what every cell must include at the start - otherwise, the results may not be correct
# 
# cleaned_data_count = spark.sql("SELECT COUNT(*) AS count FROM ratings_cleaned_Data").collect()[0]['count']
# print(cleaned_data_count)

"""Next, lets have a look at the head of the dataframe to better understand our data before diving into some analyses!"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# ratings_cleaned_df.show(5)

"""### 3.1: Positive Review-Ratios [3 pts]

Since this dataset contains reviews and a corresponding set of helpfulness scores (based on whether reviewers found the book reviews valuable), we can analyze and compare how many positive reviews each book-review received. In theory, this can act as a proxy to analyze the overall review quality for each book.

Formally, this involves the following: for every book, look at the ratio of the number of book-reviews with the total number of positive reviews of book-reviews.

**TODO**:


*   Use SparkSQL to find the ratio of the number of reviews of a book to the number of positive reviews that book received, for each book. You should be able to get this data from `ratings_cleaned_data`
*   Sort the values by `Title` name in alphabetical order
*   Round the `Ratio` to 2 decimal places
*   Save the result as `positive_review_ratio_sdf`



**Final Schema**:
>Title | Ratio
>--- | ---


**Hint**:
*  Think about which aggregate functions make most sense to use here
*  When applying aggregate functions, what function must be applied first
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# # TODO: Use Spark SQL to find the ratio of the number of book-reviews (numerator) with the total number of positive reviews of that book's reviews (denominator)
# # TODO: Sort the values by Title name in alphabetical order
# # TODO: Round the Ratio to 2 decimal places
# # TODO: Save the result as positive_review_ratio_sdf
# 
# query = '''
# SELECT
#   Title,
#   ROUND(COUNT(*)/ SUM(positive_reviews), 2) AS Ratio
# FROM
#   ratings_cleaned_data
# GROUP BY
#   Title
# ORDER BY
#   Title
# 
# '''
# 
# positive_review_ratio_sdf = spark.sql(query)

# Commented out IPython magic to ensure Python compatibility.
# #################     DO NOT EDIT      ##################
# %%spark
# positive_review_ratio_sdf.createOrReplaceTempView("test_3_1")
# test_3_1_sdf = spark.sql("SELECT * FROM test_3_1")

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o test_3_1_sdf

test_3_1_sdf

# 3 points
grader.grade(test_case_id = 'test_positive_ratio', answer = test_3_1_sdf)

"""### 3.2: Weighted Average Score per book [4 Pts]
Similar to 3.1, we are looking for another proxy to compare the usefulness of the book-reviews. This time, we will find the weighted average of the review score with its corresponding helpfulness score. Essentially, we want to know the average weighted score for each book so that we can compare the value of the reviews.

In this section, we want to only look at the best weighted scores. To do so, we will filter our results for weighted score above 2.5

**TODO**:


*   Use Spark SQL to find the average weighted product of the `score` and `helpfulness_score` as `Weighted_Score` for every book from `ratings_cleaned_data`
*   Sort the values by `Title` name in alphabetical order
*   Round the `Weighted_Score` to 1 decimal place
*   Filter for `Weighted_Score` above 2.5
*   Save the result as `weighted_score_sdf`



**Final Schema**:
>Title | Weighted_Score
>--- | ---


**Hint**:
*  Think about which aggregate functions make most sense to use here
*  How should we handle the filtering part? You're likely thinking of two methods to do this - which one makes more sense to use and why?
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# # TODO: Use Spark SQL to find the average weighted product of the score and helpfulness_score for every book
# # TODO: Sort the values by Title name in alphabetical order
# # TODO: Round the Weighted_Score to 1 decimal place
# # TODO: Filter for Weighted_Score above 2.5
# # TODO: Save the result as weighted_score_sdf
# 
# query = '''
# SELECT
#   Title,
#   ROUND(AVG(helpfulness_score * score), 1) AS Weighted_Score
# FROM
#   ratings_cleaned_data
# GROUP BY
#   Title
# HAVING
#   ROUND(AVG(helpfulness_score * score), 1) > 2.5
# ORDER BY
#   Title
# 
# '''
# 
# weighted_score_sdf = spark.sql(query)
# #weighted_score_sdf.show(30)

# Commented out IPython magic to ensure Python compatibility.
# #################     DO NOT EDIT      ##################
# %%spark
# weighted_score_sdf.createOrReplaceTempView("test_3_2")
# test_3_2_sdf = spark.sql("SELECT * FROM test_3_2")

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o test_3_2_sdf

test_3_2_sdf

# 4 points
grader.grade(test_case_id = 'test_weighted_score', answer = test_3_2_sdf)

"""### 3.3: Are longer reviews better? Worse? Lets explore! [10 pts]

####3.3.1: Length of summaries [3 pts]

Continuing on our path to find the best book-reviews based on their corresponding reviews, we now aim to see if there is a relationship between the length of a review and its helpfulness scores.

We'll start off nice and easy: lets first find the lengths of each of the reviews and include this as a new column in our dataframe.

**TODO**:

*   Use Spark SQL to find the length of each of the reviews in the `summary` column
*   Sort the values by `Summary_Length` in descending order
*   Save the result as `length_summaries_sdf`
*   Ensure that no null/empty summaries are included

**Final Schema**:
Include everything originally in `ratings_cleaned_data` with the addition of the `Summary_Length` column
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# # TODO:
# query = '''
#   SELECT
#     *,
#     LENGTH(summary) AS Summary_Length
#   FROM
#     ratings_cleaned_data
#   WHERE
#     summary is not null or summary != ''
#   ORDER BY
#     summary_length DESC
# 
# '''
# 
# length_summaries_sdf = spark.sql(query)

# Commented out IPython magic to ensure Python compatibility.
# #################     DO NOT EDIT      ##################
# %%spark
# length_summaries_sdf.createOrReplaceTempView("test_3_3_1")
# test_3_3_1_sdf = spark.sql('''SELECT * FROM test_3_3_1 LIMIT 2000''')

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o test_3_3_1_sdf

test_3_3_1_sdf

# 3 points
grader.grade(test_case_id = 'test_length_summaries', answer = test_3_3_1_sdf)

"""####3.3.2: Normalize `Summary_Length` [2 pts]

Now that we have the lengths, we need to have some form of normalization to make this a valid and fair comparison (not to penalize one more than the other).

**TODO**:

*   Use Spark SQL to normalize the `Summary_Length` column by dividing it by the length of the longest summary
*   Use the results from 3.3.1 to do this. **Do not** use `ratings_cleaned_data` (penalties apply)
*   Store the normalized value in a column named `Normalized_Summary_Length`, rounded to 3 decimal places
*   Sort the values by `Normalized_Summary_Length` in ascending order
*   Save the result as `normalized_summaries_sdf`

**Final Schema**:
Include everything originally in `length_summaries_sdf` with the addition of the `Normalized_Summary_Length` column

**Hint**: If there are Nan values in your output, revisit 3.3.1 and re-read the instructions carefully
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# # TODO:
# query = '''
#   SELECT
#     *,
#     ROUND(Summary_Length / MAX(Summary_Length) OVER(PARTITION BY Title), 3) AS Normalized_Summary_Length
#   FROM
#     test_3_3_1
#   ORDER BY
#     Normalized_Summary_Length ASC
# 
# '''
# 
# normalized_summaries_sdf = spark.sql(query)

# Commented out IPython magic to ensure Python compatibility.
# #################     DO NOT EDIT      ##################
# %%spark
# normalized_summaries_sdf.createOrReplaceTempView("test_3_3_2")
# test_3_3_2_sdf = spark.sql('''SELECT * FROM test_3_3_2''')

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o test_3_3_2_sdf

test_3_3_2_sdf

# 2 points
grader.grade(test_case_id = 'test_normalized_summaries', answer = test_3_3_2_sdf)

"""####3.3.3: Binned Average helpfulness scores [5 pts]

Great! Now lets analyze the average helpfulness scores across different ranges of normalized summary lengths. This way, we can determine how the length of the summaries impact how helpful the actual review was.

The buckets that we will use for the `Normalized_Summary_Length` are as follows (use these EXACTLY - do not remove or add any spaces, etc.):

*   0 - 0.1
*   0.1 - 0.2
*   0.2 - 0.3
*   0.3 - 0.4
*   0.4 - 0.5
*   0.5 - 0.6
*   0.6 - 0.7
*   0.7 - 0.8
*   0.8 - 0.9
*   0.9 - 1.0

**TODO**:

*   Use Spark SQL to generate a table of the average helpfulness scores among the `Normalized_Summary_Length` buckets mentioned above
*   Define a new column, `bucketed_normalization` that determines which bucket each review belongs to
*   Use the results from 3.3.2 to do this. **Do not** use `ratings_cleaned_data` or the results from 3.3.1 (penalties apply)
*   Store the average in a column named `average_helpfulness_score`, rounded to 3 decimal places
*   Sort the values by `average_helpfulness_score` in descending order
*   Save the result as `binned_helpfulness_sdf`

**Final Schema**:
>bucketed_normalization | average_helpfulness_score
>--- | ---

**Hint**: Think about how to handle NaN values (what makes most sense here and why...?)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# # TODO:
# query = '''
# WITH temp AS (
#   SELECT
#     *,
#     CASE
#       WHEN Normalized_Summary_Length >= 0 AND Normalized_Summary_Length < 0.1 THEN '0 - 0.1'
#       WHEN Normalized_Summary_Length >= 0.1 AND Normalized_Summary_Length < 0.2 THEN '0.1 - 0.2'
#       WHEN Normalized_Summary_Length >= 0.2 AND Normalized_Summary_Length < 0.3 THEN '0.2 - 0.3'
#       WHEN Normalized_Summary_Length >= 0.3 AND Normalized_Summary_Length < 0.4 THEN '0.3 - 0.4'
#       WHEN Normalized_Summary_Length >= 0.4 AND Normalized_Summary_Length < 0.5 THEN '0.4 - 0.5'
#       WHEN Normalized_Summary_Length >= 0.5 AND Normalized_Summary_Length < 0.6 THEN '0.5 - 0.6'
#       WHEN Normalized_Summary_Length >= 0.6 AND Normalized_Summary_Length < 0.7 THEN '0.6 - 0.7'
#       WHEN Normalized_Summary_Length >= 0.7 AND Normalized_Summary_Length < 0.8 THEN '0.7 - 0.8'
#       WHEN Normalized_Summary_Length >= 0.8 AND Normalized_Summary_Length < 0.9 THEN '0.8 - 0.9'
#       ELSE '0.9 - 1.0'
#     END AS bucketed_normalization
#   FROM
#     test_3_3_2
#   WHERE helpfulness_score != 'nan' or helpfulness_score is not null
#     )
# SELECT
#   bucketed_normalization,
#   ROUND(AVG(helpfulness_score), 3) AS average_helpfulness_score
# FROM temp
# GROUP BY
#   bucketed_normalization
# ORDER BY
#   average_helpfulness_score DESC
# 
# '''
# 
# 
# 
# binned_helpfulness_sdf = spark.sql(query)
# #binned_helpfulness_sdf.show(30)

# Commented out IPython magic to ensure Python compatibility.
# #################     DO NOT EDIT      ##################
# %%spark
# binned_helpfulness_sdf.createOrReplaceTempView("test_3_3_3")
# test_3_3_3_sdf = spark.sql('''SELECT * FROM test_3_3_3''')

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o test_3_3_3_sdf

test_3_3_3_sdf

# 5 points
grader.grade(test_case_id = 'test_binned_helpfulness', answer = test_3_3_3_sdf)

"""Oh wow! It seems that mid-length summaries were the most helpfuless, on average! Let's plot the data to get a better understanding of how it varies."""

# do not change this code -- this part is not graded, but just for you to see

# feel free to comment this out to help resolve large file hidden issues when submitting to Gradescope

from matplotlib import pyplot as plt
test_3_3_3_sdf.sort_values('bucketed_normalization').plot.bar(x='bucketed_normalization', y='average_helpfulness_score', legend=None)
plt.ylabel('Average Helpfulness Score')
plt.xlabel('Bucketed Normalization')
plt.title('Average Helpfulness Scores across Bucketed Normalizations')
plt.show()

"""### 3.4: Convert Unix Time! [2 pts]

As our final step in Section 3, we will convert the time column in `ratings_cleaned_data` to a DateTime variable for use in the next section.

The [`from_unixtime`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.from_unixtime.html) function should help with this

**TODO**:

*   Use Spark SQL to generate a new column
*   Define a new column, `cleaned_time` that converts the unix time (`time` column) to a DateTime object of the form "yyyy-MM-dd HH:mm:ss"
*   Store the results as `cleaned_time_reviews_sdf`

**Final Schema**: Include everything originally in `ratings_cleaned_data` with the addition of the `cleaned_time` column
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# query = '''
# SELECT
#   *,
#   FROM_UNIXTIME(time, 'yyyy-MM-dd HH:mm:ss') AS cleaned_time
# FROM
#   ratings_cleaned_data
# 
# '''
# 
# cleaned_time_reviews_sdf = spark.sql(query)

# Commented out IPython magic to ensure Python compatibility.
# #################     DO NOT EDIT      ##################
# %%spark
# cleaned_time_reviews_sdf.createOrReplaceTempView("test_3_4")
# test_3_4_sdf = spark.sql('''SELECT * FROM test_3_4''')

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o test_3_4_sdf

test_3_4_sdf

# 2 points
grader.grade(test_case_id = 'test_unix_time', answer = test_3_4_sdf)

"""## Step 4: Combining Books and Reviews Data [7 pts]

Congrats on making it this far!

This section will focus on combining the book data from part 2 and reviews data from part 3 (so fun!)

We want to use the dataset from 3.4, which cleaned the `time` column. Run the following cell to enable its use in Spark
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# cleaned_time_reviews_sdf.createOrReplaceTempView("ratings_cleaned_time_data")

"""Here, we will join `books_cleaned_data` with `ratings_cleaned_time_data` on `Title` and store the result in books_reviews_data"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# query = '''
# SELECT *
# FROM books_cleaned_data AS b JOIN ratings_cleaned_time_data AS r ON b.Title = r.Title
# '''
# books_reviews_sdf = spark.sql(query)

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# books_reviews_sdf.createOrReplaceTempView("books_reviews_data")
# #books_reviews_sdf.show(5)

"""### 4.1 - Count number of reviews per author [3 pts]

`ratings_cleaned_time_data` contains a row for each review on a title. `books_cleaned_data` contains a row for each unique combination of publisher, book, and category.

We want to count the number of reviews associated with each author. Note: we will not be using the `no_of_reviews` column in this part. We are just trying to see the number of rows in `ratings_cleaned_time_data` that correspond to an author in `books_cleaned_data`.

**TODO**:


*   Use Spark SQL to find the total reviews associated with each author. Use `books_reviews_data`.
*   Sort the values by `review_count` in descending order and authors in alphabetical order
*   Save the result as `reviews_count_sdf`



**Final Schema**:
>authors | review_count
>--- | ---


**Hint**:
*  What value is unique to a review?
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# query = '''
# SELECT authors, COUNT(DISTINCT text) as review_count
# FROM books_reviews_data
# GROUP BY authors
# ORDER BY review_count DESC, authors
# 
# '''
# reviews_count_sdf = spark.sql(query)
# #reviews_count_sdf.show(30)

# Commented out IPython magic to ensure Python compatibility.
# #################     DO NOT EDIT      ##################
# %%spark
# reviews_count_sdf.createOrReplaceTempView("test_4_1")
# test_4_1_sdf = spark.sql('''SELECT * FROM test_4_1''')

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o test_4_1_sdf

grader.grade(test_case_id = 'test_reviews_count', answer = test_4_1_sdf)

"""### 4.2 - Most infuential authors between 2000 - 2005 [4 pts]

The `no_of_reviews` column tells you the number of people who responded to a particular book review. Think of it as the number of reviews for a review.

We want to measure the "influence" of an author by summing up these reviews of reviews to see how many total people have held opinions about a book. In this part, you will return the most influential author for each year from 2000-2005 (inclusive).

**TODO**:


*   Use Spark SQL and `books_reviews_data` to get the author with the most cumulative `no_of_reviews` (called `total_reviews`) per year.
*   Sort the values by `year` in ascending order
*   Save the result as `reviews_count_sdf`


**Final Schema**:
>year | authors | total_reviews
>--- | --- | ---

There should be six rows in the final result, each corresponding to a year.

**Hints**:
*  This is similar to part 2.2!
*  The YEAR() function can be used to get the year of a SQL datetime object
"""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# query = '''
# WITH year_author_reviews AS (
#   SELECT YEAR(cleaned_time) as year,
#   authors,
#   SUM(no_of_reviews) as total_reviews
#   FROM books_reviews_data
#   WHERE YEAR(cleaned_time) BETWEEN 2000 AND 2005
#   GROUP BY year, authors
# ),
# ranked_authors AS (
#   SELECT year,
#     authors,
#     total_reviews,
#     RANK() OVER (PARTITION BY year ORDER BY total_reviews DESC) as rank
#   FROM year_author_reviews
# )
# SELECT year,
#   authors,
#   total_reviews
# FROM ranked_authors
# WHERE rank = 1
# ORDER BY year
# 
# '''
# influential_authors_sdf = spark.sql(query)
# #influential_authors_sdf.show()

# Commented out IPython magic to ensure Python compatibility.
# #################     DO NOT EDIT      ##################
# %%spark
# influential_authors_sdf.createOrReplaceTempView("test_4_2")
# test_4_2_sdf = spark.sql("SELECT * FROM test_4_2")

# Commented out IPython magic to ensure Python compatibility.
#Convert to Pandas
# %spark -o test_4_2_sdf

## AUTOGRADER
grader.grade(test_case_id = 'test_influential_authors', answer = test_4_2_sdf)

"""## Step 5: Star Wars: Revenge of the Social Network [28 Pts]

In this section, we will be working with a dataset representing interactions between Star Wars characters from the film **[Star Wars: Episode I – The Phantom Menace](https://en.wikipedia.org/wiki/Star_Wars:_Episode_I_%E2%80%93_The_Phantom_Menace)**. In our table, each row represents an interaction between `Character1` and `Character2` from the film.

Let's introduce a fun little concept called the Bacon Number! The Bacon number of an actor or actress is the number of degrees of separation they have from actor Kevin Bacon, as defined by the game known as [**Six Degrees of Kevin Bacon**](https://en.wikipedia.org/wiki/Six_Degrees_of_Kevin_Bacon). For example, Kevin Bacon's Bacon number is 0. If an actor works in a movie with Kevin Bacon, the actor's Bacon number is 1. If an actor works with an actor who worked with Kevin Bacon in a movie, the first actor's Bacon number is 2, and so forth.

How do we implement the *Super Bacon* score for our dataset though? We define this number as follows: if Person A follows Person B, and Person B follows Person C, then the *Super Bacon* score of C with respect to A will be 2.

To calculate this number, we'll use the concepts of graphs and BFS!
"""

!pip install pyspark

from pyspark.sql import SparkSession

appName = "PySpark"

# Create Spark session
spark = SparkSession.builder.appName(appName).getOrCreate()

"""### 5.1: “Traversing” a Graph [8 Pts]

Let's review how BFS works!
"""

import pandas as pd
from IPython.display import Image as I

bfsgif =\
'https://upload.wikimedia.org/wikipedia/commons/5/5d/Breadth-First-S'+\
'earch-Algorithm.gif'
dfsgif=\
'https://upload.wikimedia.org/wikipedia/commons/7/7f/Depth-First-Search.gif'

"""#### Intro to Breadth-First Search (BFS)


To start off, we will be implementing a graph traversal algorithm known as Breadth First Search. It works in a way that's equivalent to how a stain spreads on a white t-shirt. Take a look at the graph below:

<p align = "center">
<img src = "https://imgur.com/WU3AUwg.png" width= "600" align ="center"/>

* Consider starting BFS from point A (green). This is considered the starting frontier/singular **origin node**.
* The first round of BFS would involve finding all the nodes directly reachable from A, namely B-F (blue circles). These blue nodes make up the next frontier at depth 1 away from our starting node A.
* The second round would then be identifying the red nodes which are the neighbors of the blue nodes. Now, the red nodes all belong to a frontier 2 depth away from A. Note that node A is also a neighbor of a blue node. However, since it has already been visited, it does not get added to this frontier.

This process continues until all the nodes in the graph have been visited.
If you would like to learn more about BFS, we highly suggest looking [here](https://www.tutorialspoint.com/data_structures_algorithms/breadth_first_traversal.htm). **We also recommend watching and reviewing Recitation #6 material if you want a refresher on BFS.**

We will now be implementing `spark_bfs(G, N, d)`, our Spark flavor of BFS that takes:

1.   graph **G**
2.   set of origin nodes **N**
3.   max depth **d**.

In order to write a successful BFS function, you are going to need to figure out
1. How to keep track of nodes that we have visited
2. How to properly find all the nodes at the next depth
3. How to avoid cycles and ensure that we do not constantly loop through the same edges (take a look at J-K in the figure above)

#### BFS vs. DFS Animation
Run the code cells below to understand the difference between depth and breadth first search! (Source: Wikimedia Commons)
"""

# NOT GRADED
print('BFS:')
I(url=bfsgif)

# NOT GRADED
print('DFS:')
I(url=dfsgif)

"""#### 5.1.1 Implement One BFS Traversal [6 Pts]

To break down this process, let's think about how we would implement a single traversal of the graph. Given the origin node, how are we going to get to the first wave of reachable nodes (i.e. nodes with distance 1 from the origin)?

Consider the simple example graph (**G**) below:
"""

spark.conf.set("spark.sql.execution.arrow.enabled", "true")

simple = [('Arnav', 'Karan'),
          ('Arnav', 'Jeff'),
          ('Arnav', 'Federico'),
          ('Jeff', 'Liang'),
          ('Liang', 'Arnav'),
          ('Karan', 'Nicky'),
          ('Nicky', 'Yash'),
          ('Federico', 'Emily')]

simple_dict = {'from_node': ['Arnav', 'Arnav', 'Arnav', 'Jeff', 'Liang', 'Karan', 'Nicky', 'Federico'],
       'to_node': ['Karan', 'Jeff', 'Federico', 'Liang', 'Arnav', 'Nicky', 'Yash', 'Emily']}

simple_graph_df = pd.DataFrame.from_dict(simple_dict)
simple_graph_sdf = spark.createDataFrame(simple_graph_df)
#simple_graph_sdf.show()

"""As you can see, each row of this dataframe represents an edge between two nodes. Here, we are looking at a **directed** graph, which means that Arnav --> Karan does not represent the same edge as Karan --> Arnav.

Let's define our starting node as follows:
"""

smallOrig = [{'node': 'Arnav'}]

"""Then, we perform BFS with graph **G**, starting from smallOrig (**N**) to depth (**d**) 1. If we implemented `spark_bfs(G, smallOrig, 1)`, the output would look like this:"""

simple_1_round_dict = {'node': ['Karan', 'Federico', 'Jeff', 'Arnav'], 'distance': [1, 1, 1, 0]}
simple_1_round_bfs_df = pd.DataFrame.from_dict(simple_1_round_dict)
simple_1_round_bfs_sdf = spark.createDataFrame(simple_1_round_bfs_df)
#simple_1_round_bfs_sdf.show()

"""As you can see, this DataFrame logs each node with its corresponding distance away from Arnav. Moreover, we also know that these nodes are **visited**.

Hopefully, you can see how we can use our original graph and this new information to find the nodes at depth two.

This is exactly what we will try to accomplish with **spark_bfs_1_round(visited_nodes)** which will ultimately be the inner function of **spark_bfs** that we use to perform exactly one traversal of a graph.

**TODO**: Write **spark_bfs_1_round(visited_nodes)** that takes the currently dataframe of `visited_nodes`, performs one round of BFS, and returns an updated visited nodes dataframe.

*   Your input and output `visited_nodes` should have the columns *node* and *distance*

**Note**: You should assume that a temporary sdf **G** already exists. In other words, you can have access to **G** in your function.

**Hint:** It would very helpful to iterate one round of BFS using paper and pencil on `simple_1_round_bfs_sdf` with the given parameters in order to understand the problem.
"""

from pyspark.sql.types import StructType, StructField, DataType, StringType, IntegerType

def spark_bfs_1_round(visited_nodes):
  """
  :param visited_nodes: dataframe with columns node and distance
  :return: dataframe of updated visited nodes, with columns node and distance
  """
  # TODO: Complete this function to implement 1 round of BFS

  # (1) From nodes in visited_nodes, go to the neighboring nodes
  # and incremement distance by 1


  #simple_graph_sdf.show()
  max_dist = max([x[0] for x in visited_nodes.select('distance').collect()])
  max_dist_p1 = max_dist+1
  all_nodes = list(set([x[0] for x in g.select('to_node').collect()]))

  visited_nodes_sdf = visited_nodes.filter(visited_nodes.distance == max_dist)
  visited_nodes_t = list(set([x[0] for x in visited_nodes_sdf.select('node').collect()]))

  to_node1 = list(set([x[0] for x in g.filter(g.from_node.isin(visited_nodes_t)).select('to_node').collect()]))
  #distance1 = [x[0]+1  for x in visited_nodes.filter(visited_nodes.node.isin(visited_nodes_t)).select('distance').collect()][0]

  # (2) Remove duplicate entries, i.e. rows with the same node and distance
  #print(to_node1)
  #print(max_dist_p1)
  for i in to_node1:
    if i in [x[0] for x in visited_nodes.select('node').collect()]:
      if visited_nodes.filter(visited_nodes.node.contains(i)).select('distance').collect()[0][0] <= max_dist_p1:
       # visited_nodes = visited_nodes.filter(visited_nodes.node != i)
       continue
      else:
       # print(i)
        #print(max_dist_p1)
        temp_list = {'node': i, 'distance': max_dist_p1}
        temp_list = [[i, max_dist_p1]]
        schema_temp = StructType([StructField("node", StringType()), StructField("distance", IntegerType()),])
        temp_df = spark.createDataFrame(temp_list, schema_temp)
        visited_nodes = temp_df.union(visited_nodes)
    else:
      #print(i)
      #print(max_dist_p1)
      temp_list = {'node': i, 'distance': max_dist_p1}
      temp_list = [[i, max_dist_p1]]
      schema_temp = StructType([StructField("node", StringType()), StructField("distance", IntegerType()),])
      temp_df = spark.createDataFrame(temp_list, schema_temp)
      visited_nodes = temp_df.union(visited_nodes)
  return visited_nodes





  # (3) If there are two or more visited nodes with different distances, we want to
  # keep the minimum distance

schema_g = simple_graph_sdf.schema
g_pd = simple_graph_sdf.toPandas()
g = spark.createDataFrame(g_pd, schema = schema_g)

spark_bfs_1_round(simple_1_round_bfs_sdf)

"""Now, run the inner function on `simple_1_round_bfs_sdf` (i.e. result of 1 round of BFS on the simple graph) and store the results in simple_bfs_result. This is ultimately what the output of BFS to depth 2 should look like. Create your temporary sdf **G** and then call the `spark_bfs_1_round` function you created above.

**Note:** Your output should only have minimum distances and each node should only appear once.
"""

# TODO: Run spark_bfs_1_round on simple_1_round_bfs_sdf
simple_bfs_result = spark_bfs_1_round(simple_1_round_bfs_sdf)

"""**TODO**:

Convert this result to Pandas, sort by distance column in ascending order, and submit it to the autograder.

**Hint**: Check out the `toPandas()` function [documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html).
"""

# TODO: Convert simple_bfs_result to Pandas and sort by distance ascending order
simple_bfs_test = simple_bfs_result.toPandas().sort_values(by = 'distance')

### DO NOT EDIT
simple_bfs_test = simple_bfs_test.reset_index(drop=True)

#simple_bfs_test

# 6 points
grader.grade(test_case_id = 'checksimpleBFS', answer = simple_bfs_test)

"""#### 5.1.2 Full BFS Implementation [2 Pts]

Now, we will fully implement **spark_bfs**. This function should iteratively call your implemented version of **spark_bfs_1_round** and ultimately return the output of this function at **max_depth**.

You are also responsible for initializing the starting dataframe, that is converting the list of origin nodes into a spark dataframe with the nodes logged at distance 0.

Consider the following:

```
schema = StructType([
            StructField("node", StringType(), True)
        ])

    my_sdf = spark.createDataFrame(origins, schema)
```

The schema ultimately specifies the structure of the Spark DataFrame with a string `node` column. It then calls **spark.createDataFrame** to map this schema to the **origins** nodes. Also, you are responsible for ensuring that a view of your graph is available within this function. (Note: you will also need to add in a distance column)

**TODO:** Implement `spark_bfs(G,origins,max_depth)`. In the coming sections, you will run this function on `star_wars_sdf` that you will initialize in 5.2.

**Note:** you may want to run tests on the **simple_graph** example as the `star_wars_sdf` will take quite some time to run.

Also remember that in section 5.1.1, we had assumed that the temporary view already exists. So you will have to take care of creating that within this function.

These imports might be useful:
`from pyspark.sql.types import StructType, StructField, StringType, IntegerType`
"""

from pyspark.sql.types import StructType, StructField, StringType, IntegerType
# TODO: iterative search over undirected graph
# NOTE: This function will also be used in 5.3

def spark_bfs(G, origins, max_depth):
  """ runs distributed BFS to a specified max depth

  :param G: graph dataframe from 5.1.1
  :param origins: list of origin nodes stored as {"node": nodeValue}
  :param max_depth: integer value of max depth to run BFS to
  :return: dataframe with columns node, distance of all visited nodes
  """

  # (1) Define schema and load data
  schema = StructType([
            StructField("node", StringType(), True),
            StructField("distance", IntegerType(), True)
        ])


  #origins = origins[0]
  #len_o = len(origins['node'])
  #origins['distance'] = [0 for a in range(len_o)]
  origins = [[origins[0]['node'], 0]]

  # my_sdf = spark.createDataFrame(origins, schema)



  # (2) Initialize nodes_visited
  nodes_visited = spark.createDataFrame(origins, schema)

  # (3) Iterate BFS
  for a in range(max_depth):
   # nodes_visited.show(10)
    nodes_visited = spark_bfs_1_round(nodes_visited)

  return nodes_visited

"""Test that this function works on a simple example."""

### SELF CHECK: You will know your spark_bfs works if it passes autograder

### DO NOT EDIT
simple_bfs_iterative_result = spark_bfs(simple_graph_sdf, smallOrig, 3)
#simple_bfs_iterative_result.show()

# 2 points
grader.grade(test_case_id = 'iterBFS', answer = simple_bfs_iterative_result.toPandas())

"""### 5.2: In a Network Far, Far Away [8 Pts]

**Read:** Download the `star_wars.csv` dataset from [here](https://drive.google.com/file/d/1d8t3AJ1WnF4XzeJsDx6g4q2lgOgHjsCT/view?usp=sharing), and upload it to the directory in Google Drive where you are storing your HW3 notebook.

#### 5.2.1: Loading and Viewing Data

TODO: Mount your Google Drive and then edit the directory link to where you are storing the `star_wars.csv`.
"""

# mount your Google Drive
from google.colab import drive
drive.mount('/content/drive')

# TODO: modify the directory to where star_wars.csv is stored
path = "/content/drive/MyDrive/BigData/Homeworks/Homework_3/star_wars.csv" # path of file in Google Drive
star_wars_sdf = spark.read.csv(path, header=True)

"""Let's load the data to `star_wars_sdf` and see how it looks."""

# display star_wars_sdf
#star_wars_sdf.show()

"""#### 5.2.2: Characters with the Most Interactions with Others [6 Pts]

As we can see, the DataFrame consists of 2 columns named Character1 and Character1. These columns indicate an edge in the social network graph formed when two characters directly interact with each other in the film. Each row in the dataframe indicates that Character1 interacts with Character2.

**READ: For this subproblem (5.2.2) ONLY**, we are going to assume that each interaction or link is **undirected**, meaning the source or target nodes are arbitrarily assigned and that A --> B is the same as B --> A.

We want to find out which character interacted with the most number of unique characters in the film.

***TODO***:
1.   Create a DataFrame with the columns *Character* and *Unique_Interactions* from `star_wars_sdf`, in which *Character* is the name of the character and *Unique_Interactions* represents the number of unique chracters that the given character had interacted with.  
2.   The final DataFrame `count_sdf` should be sorted in descending order based on Interactions_Count.
3.   Convert `count_sdf` to a Pandas DataFrame `count_df`.

**Hint #1**: Each interaction is **undirected**, meaning it is a two-way relationship. Think about what that means for your calculation.

**Hint #2**: You may find the `UNION ALL` command useful.

**Hint #3**: Refer to HW2 if you get stuck!
"""

# TODO: Design your query

star_wars_sdf.createOrReplaceTempView('interactions')

query = """
WITH temp AS (
  SELECT Character1 as Character_f, Character2 as Character_s
  FROM interactions
  UNION ALL
  SELECT Character2 as Character_f, Character1 as Character_s
  FROM interactions)
SELECT Character_f AS Character,
  COUNT(DISTINCT Character_s) AS Unique_Interactions
FROM temp
GROUP BY Character_f
ORDER BY Unique_Interactions DESC

 """


count_sdf = spark.sql(query)
#count_sdf.show()

# TODO: Convert your count_sdf results to Pandas dataframe count_df
count_df = count_sdf.toPandas()

# display top ten characters with the most interactions
#count_df.head(10)

# 6 points
grader.grade(test_case_id = 'checkInteractionsCount', answer = count_df)

"""#### 5.2.3: Convert Data into Graph [2 Pts]

**READ: For this subproblem (5.2.3) ONLY**, we are going to assume that each interaction or link is **directed**, meaning the source or target nodes are intenionally assigned and that A --> B is **NOT** the same as B --> A. `Character1` will be the source node and `Character2` will be the target node.

Using `star_wars_sdf`, let's convert it to a graph sdf.

**TODO:** From **star_wars_graph_sdf**, rename columns to `from_node` and `to_node`.

*   `from_node` has all the entries from the `Character1` column
*    `to_node` has all the entries from the `Character2` column
"""

# TODO: using star_wars_df, create star_wars_sdf with the appropriate columns
star_wars_sdf.createOrReplaceTempView('interactions')

query = """
SELECT
  Character1 AS from_node,
  Character2 AS to_node
FROM interactions
"""

star_wars_graph_sdf = spark.sql(query)
#star_wars_graph_sdf.show()

# 4 points
grader.grade(test_case_id = 'checkStarWarsGraph', answer = star_wars_graph_sdf.toPandas())

"""### 5.3: Boss Nass Search [12 Pts]

Let's find out the Super Bacon score with respect to the character `BOSS NASS`. You will use the `spark_bfs` that you created earlier in section 5.1.2 for this problem.

**TODO:**
*   Initialize your orignal node with variable `orig`
*   Run your spark_bfs function with the correct arguments. Sort the resulting table in descending order based on the column `distance`.
    *   The `orderBy` [function](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.orderBy.html) may be helpful.
*   Save your final table as `bfs_boss_nass_sdf`.
*   Convert `bfs_boss_nass_sdf` to a Pandas DataFrame `bfs_boss_nass_df`.
*   Save the character with the highest score as a string in the variable `highest_bacon_scorer`. **Do not hardcode your answer.**
"""

schema_g = star_wars_graph_sdf.schema
g_pd = star_wars_graph_sdf.toPandas()
g = spark.createDataFrame(g_pd, schema = schema_g)

# TODO: initialize your origin node
# {"node": nodeValue}
orig = [{"node": "BOSS NASS"}]

# 2 points
grader.grade(test_case_id = 'checkBFS1', answer = orig)

### RUN CELL, DO NOT EDIT
depth = 4

# TODO: Perform BFS using spark_bfs for the given amount of depth, the star_wars_graph_sdf, and the origin node
# save results in bfs_boss_nass_sdf

bfs_boss_nass_sdf = spark_bfs(g, orig, depth)
#bfs_boss_nass_sdf.show()

# TODO: Convert to Pandas dataframe
bfs_boss_nass_df = bfs_boss_nass_sdf.toPandas()
#bfs_boss_nass_df

# 6 points
grader.grade(test_case_id = 'checkBFS2', answer = bfs_boss_nass_df)

# display top 5 rows
#bfs_boss_nass_df.head(5)

# TODO: Report and save the character (string) with the highest score in highest_bacon_scorer
# Do not hardcode your answer

highest_bacon_scorer = bfs_boss_nass_df.iloc[0]['node']
highest_bacon_scorer

# 4 points
grader.grade(test_case_id = 'checkBFS3', answer = highest_bacon_scorer)